# PIPELINE DEFINITION
# Name: performance-predictor-stacked-training-pipeline
# Description: Pipeline definition for training, evaluation, and deployment
# Inputs:
#    dataset_uri: str
#    model_repo: str
#    project_id: str
#    thresholds_dict_str: str
#    trigger_id: str
# Outputs:
#    model-evaluation-kpi: system.Metrics
#    model-evaluation-metrics: system.ClassificationMetrics
components:
  comp-condition-1:
    dag:
      tasks:
        run-build-trigger:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-run-build-trigger
          dependentTasks:
          - upload-model-to-gcs
          inputs:
            parameters:
              project_id:
                componentInputParameter: pipelinechannel--project_id
              trigger_id:
                componentInputParameter: pipelinechannel--trigger_id
          taskInfo:
            name: run-build-trigger
        upload-model-to-gcs:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-upload-model-to-gcs
          inputs:
            artifacts:
              model:
                componentInputArtifact: pipelinechannel--train-stacked-model-model
            parameters:
              model_repo:
                runtimeValue:
                  constant: models_de2024_17
              project_id:
                runtimeValue:
                  constant: de2024-436414
          taskInfo:
            name: upload-model-to-gcs
    inputDefinitions:
      artifacts:
        pipelinechannel--train-stacked-model-model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
      parameters:
        pipelinechannel--model-evaluation-approval:
          parameterType: BOOLEAN
        pipelinechannel--project_id:
          parameterType: STRING
        pipelinechannel--trigger_id:
          parameterType: STRING
  comp-importer:
    executorLabel: exec-importer
    inputDefinitions:
      parameters:
        uri:
          parameterType: STRING
    outputDefinitions:
      artifacts:
        artifact:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
  comp-model-evaluation:
    executorLabel: exec-model-evaluation
    inputDefinitions:
      artifacts:
        model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
        test_set:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
      parameters:
        thresholds_dict_str:
          parameterType: STRING
    outputDefinitions:
      artifacts:
        kpi:
          artifactType:
            schemaTitle: system.Metrics
            schemaVersion: 0.0.1
        metrics:
          artifactType:
            schemaTitle: system.ClassificationMetrics
            schemaVersion: 0.0.1
      parameters:
        approval:
          parameterType: BOOLEAN
  comp-run-build-trigger:
    executorLabel: exec-run-build-trigger
    inputDefinitions:
      parameters:
        project_id:
          parameterType: STRING
        trigger_id:
          parameterType: STRING
  comp-train-stacked-model:
    executorLabel: exec-train-stacked-model
    inputDefinitions:
      artifacts:
        features:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
    outputDefinitions:
      artifacts:
        model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
  comp-train-test-split:
    executorLabel: exec-train-test-split
    inputDefinitions:
      artifacts:
        dataset:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
    outputDefinitions:
      artifacts:
        dataset_test:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        dataset_train:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
  comp-upload-model-to-gcs:
    executorLabel: exec-upload-model-to-gcs
    inputDefinitions:
      artifacts:
        model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
      parameters:
        model_repo:
          parameterType: STRING
        project_id:
          parameterType: STRING
deploymentSpec:
  executors:
    exec-importer:
      importer:
        artifactUri:
          runtimeParameter: uri
        typeSchema:
          schemaTitle: system.Dataset
          schemaVersion: 0.0.1
    exec-model-evaluation:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - model_evaluation
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.7.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'pandas' 'scikit-learn==1.3.2'\
          \ 'numpy' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef model_evaluation(test_set: Input[Dataset], model: Input[Model],\
          \ thresholds_dict_str: str, metrics: Output[ClassificationMetrics], kpi:\
          \ Output[Metrics]) -> NamedTuple('outputs', [('approval', bool)]):\n   \
          \ '''Evaluates the model and approves it if accuracy meets the threshold'''\n\
          \    import pandas as pd\n    import pickle\n    from sklearn.metrics import\
          \ accuracy_score, roc_curve, confusion_matrix\n    import json\n    from\
          \ numpy import nan_to_num\n\n    # Load test data\n    data = pd.read_csv(test_set.path\
          \ + \".csv\")\n    X_test = data.drop(columns=[\"class\"])\n    y_test =\
          \ data[\"class\"]\n\n    # Load model\n    with open(model.path + \".pkl\"\
          , 'rb') as f:\n        pipeline = pickle.load(f)\n\n    # Make predictions\
          \ and evaluate\n    y_pred = pipeline.predict(X_test)\n    accuracy = accuracy_score(y_test,\
          \ y_pred)\n\n    # Log ROC curve\n    y_proba = pipeline.predict_proba(X_test)[:,\
          \ 1]\n    fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n    metrics.log_roc_curve(fpr.tolist(),\
          \ tpr.tolist(), thresholds.tolist())\n\n    # Log Confusion Matrix\n   \
          \ metrics.log_confusion_matrix(['Negative', 'Positive'], confusion_matrix(y_test,\
          \ y_pred).tolist())\n\n    # Accuracy metric\n    kpi.log_metric(\"accuracy\"\
          , accuracy)\n    threshold = json.loads(thresholds_dict_str)[\"roc\"]\n\
          \    return (accuracy >= threshold, )\n\n"
        image: python:3.10.7-slim
    exec-run-build-trigger:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - run_build_trigger
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.7.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'google-cloud-build'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef run_build_trigger(project_id:str, trigger_id:str):\n    import\
          \ sys\n    from google.cloud.devtools import cloudbuild_v1    \n    import\
          \ logging \n    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\
          \ \n\n    # Create a client\n    client = cloudbuild_v1.CloudBuildClient()\n\
          \    name = f\"projects/{project_id}/locations/us-central1/triggers/{trigger_id}\"\
          \n    # Initialize request argument(s)\n    request = cloudbuild_v1.RunBuildTriggerRequest(\
          \        \n        project_id=\"de2024-436414\",\n        trigger_id=\"\
          1d9db7f0-bd68-4559-8b42-7b6812646cd7\",\n        name=name\n    )\n\n  \
          \  # Make the request\n    operation = client.run_build_trigger(request=request)\n\
          \n    logging.info(\"Trigger the CI-CD Pipeline: \" + trigger_id)\n\n"
        image: python:3.10.7-slim
    exec-train-stacked-model:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - train_stacked_model
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.7.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'pandas' 'scikit-learn==1.3.2'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef train_stacked_model(features: Input[Dataset], model: Output[Model]):\n\
          \    '''Train a stacking classifier with SVC and LogisticRegression'''\n\
          \    import pandas as pd\n    from sklearn.compose import ColumnTransformer\n\
          \    from sklearn.ensemble import StackingClassifier\n    from sklearn.linear_model\
          \ import LogisticRegression\n    from sklearn.svm import SVC\n    from sklearn.pipeline\
          \ import Pipeline\n    from sklearn.preprocessing import OneHotEncoder,\
          \ StandardScaler\n    import pickle\n\n    # Load the dataset\n    data\
          \ = pd.read_csv(features.path + \".csv\")\n    categorical_columns = ['schoolsup',\
          \ 'higher']\n    numerical_columns = ['absences', 'failures', 'Medu', 'Fedu',\
          \ 'Walc', 'Dalc', 'famrel', 'goout', 'freetime', 'studytime']\n\n    # Preprocessing\
          \ pipeline\n    preprocessor = ColumnTransformer(\n        transformers=[\n\
          \            ('num', StandardScaler(), numerical_columns),\n           \
          \ ('cat', OneHotEncoder(drop='if_binary'), categorical_columns)\n      \
          \  ])\n\n    # Define base models and stacking ensemble\n    svm_linear\
          \ = SVC(C=0.1, kernel='linear', probability=True)\n    svm_rbf = SVC(C=0.1,\
          \ kernel='rbf', probability=True)\n    meta_model = LogisticRegression(C=10)\n\
          \n    # Stacking ensemble\n    stack_clf = StackingClassifier(\n       \
          \ estimators=[('svm_linear', svm_linear), ('svm_rbf', svm_rbf)],\n     \
          \   final_estimator=meta_model,\n        cv=10\n    )\n\n    # Full pipeline\n\
          \    pipeline = Pipeline(steps=[\n        ('preprocessor', preprocessor),\n\
          \        ('classifier', stack_clf)\n    ])\n\n    # Train model\n    X =\
          \ data[numerical_columns + categorical_columns]\n    y = data['class']\n\
          \    pipeline.fit(X, y)\n\n    # Save model\n    with open(model.path +\
          \ \".pkl\", 'wb') as f:\n        pickle.dump(pipeline, f)\n\n"
        image: python:3.10.7-slim
    exec-train-test-split:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - train_test_split
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.7.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'pandas' 'scikit-learn==1.3.2'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef train_test_split(dataset: Input[Dataset], dataset_train: Output[Dataset],\
          \ dataset_test: Output[Dataset]):\n    '''Splits dataset into train and\
          \ test sets'''\n    import pandas as pd\n    from sklearn.model_selection\
          \ import train_test_split as tts\n\n    df = pd.read_csv(dataset.path)\n\
          \    train, test = tts(df, test_size=0.2, random_state=42)\n    train.to_csv(dataset_train.path\
          \ + \".csv\", index=False)\n    test.to_csv(dataset_test.path + \".csv\"\
          , index=False)\n\n"
        image: python:3.10.7-slim
    exec-upload-model-to-gcs:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - upload_model_to_gcs
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.7.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'google-cloud-storage'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef upload_model_to_gcs(project_id: str, model_repo: str, model:\
          \ Input[Model]):\n    '''Uploads model to Google Cloud Storage'''\n    from\
          \ google.cloud import storage\n\n    client = storage.Client(project=project_id)\n\
          \    bucket = client.bucket(model_repo)\n    blob = bucket.blob('stacked_model.pkl')\n\
          \    blob.upload_from_filename(model.path + '.pkl')\n\n"
        image: python:3.10.7-slim
pipelineInfo:
  description: Pipeline definition for training, evaluation, and deployment
  name: performance-predictor-stacked-training-pipeline
root:
  dag:
    outputs:
      artifacts:
        model-evaluation-kpi:
          artifactSelectors:
          - outputArtifactKey: kpi
            producerSubtask: model-evaluation
        model-evaluation-metrics:
          artifactSelectors:
          - outputArtifactKey: metrics
            producerSubtask: model-evaluation
    tasks:
      condition-1:
        componentRef:
          name: comp-condition-1
        dependentTasks:
        - model-evaluation
        - train-stacked-model
        inputs:
          artifacts:
            pipelinechannel--train-stacked-model-model:
              taskOutputArtifact:
                outputArtifactKey: model
                producerTask: train-stacked-model
          parameters:
            pipelinechannel--model-evaluation-approval:
              taskOutputParameter:
                outputParameterKey: approval
                producerTask: model-evaluation
            pipelinechannel--project_id:
              componentInputParameter: project_id
            pipelinechannel--trigger_id:
              componentInputParameter: trigger_id
        taskInfo:
          name: condition-1
        triggerPolicy:
          condition: inputs.parameter_values['pipelinechannel--model-evaluation-approval']
            == true
      importer:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-importer
        inputs:
          parameters:
            uri:
              componentInputParameter: dataset_uri
        taskInfo:
          name: importer
      model-evaluation:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-model-evaluation
        dependentTasks:
        - train-stacked-model
        - train-test-split
        inputs:
          artifacts:
            model:
              taskOutputArtifact:
                outputArtifactKey: model
                producerTask: train-stacked-model
            test_set:
              taskOutputArtifact:
                outputArtifactKey: dataset_test
                producerTask: train-test-split
          parameters:
            thresholds_dict_str:
              componentInputParameter: thresholds_dict_str
        taskInfo:
          name: model-evaluation
      train-stacked-model:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-train-stacked-model
        dependentTasks:
        - train-test-split
        inputs:
          artifacts:
            features:
              taskOutputArtifact:
                outputArtifactKey: dataset_train
                producerTask: train-test-split
        taskInfo:
          name: train-stacked-model
      train-test-split:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-train-test-split
        dependentTasks:
        - importer
        inputs:
          artifacts:
            dataset:
              taskOutputArtifact:
                outputArtifactKey: artifact
                producerTask: importer
        taskInfo:
          name: train-test-split
  inputDefinitions:
    parameters:
      dataset_uri:
        parameterType: STRING
      model_repo:
        parameterType: STRING
      project_id:
        parameterType: STRING
      thresholds_dict_str:
        parameterType: STRING
      trigger_id:
        parameterType: STRING
  outputDefinitions:
    artifacts:
      model-evaluation-kpi:
        artifactType:
          schemaTitle: system.Metrics
          schemaVersion: 0.0.1
      model-evaluation-metrics:
        artifactType:
          schemaTitle: system.ClassificationMetrics
          schemaVersion: 0.0.1
schemaVersion: 2.1.0
sdkVersion: kfp-2.7.0
